{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this scripts is intended to do a simple tests for L-BFGS to update P wave velocity, if you want to update S, please change the time-window selection scheme to capture the S phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####run the jupyter notebook for the tomography model \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT='/data2/yjgao/data/DATASET/package_forthesis/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_forward(dir_run,dir_event,simtype,modelfile,dir_storesyn):\n",
    "    os.chdir(dir_run)\n",
    "    os.chdir(dir_event)\n",
    "    from shutil import copyfile\n",
    "    from distutils.dir_util import copy_tree\n",
    "    copyfile(modelfile,'DATA/profile.xyz')\n",
    "    copyfile('../run_this_example.sh','run_this_example.sh')\n",
    "    os.system('ln -s specfem2d-devel/utils/change_simulation_type.pl')\n",
    "    os.system('perl change_simulation_type.pl -f') \n",
    "    print('start forward modelling',dir_event)\n",
    "    os.system('bash run_this_example.sh')\n",
    "    print('finished forward modelling')\n",
    "    ###store the synthetics to the dir observed or syn for backup\n",
    "    copy_tree('OUTPUT_FILES',dir_storesyn)\n",
    "    os.chdir(dir_run)\n",
    "    #print(pwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###here simulate the target model or the observed data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfile=PROJECT+'MODELS/profile_obs.xyz'\n",
    "from datetime import datetime\n",
    "start=datetime.now()\n",
    "launch_forward(PROJECT,'EVENT1','forward',modelfile,'REF_SEM')\n",
    "print (datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfile=PROJECT+'MODELS/profile_obs_smooth.xyz'\n",
    "from datetime import datetime\n",
    "start=datetime.now()\n",
    "launch_forward(PROJECT,'EVENT1','forward',modelfile,'REF_SEM_smooth')\n",
    "print (datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfile=PROJECT+'MODELS/profile_obs.xyz'\n",
    "from datetime import datetime\n",
    "start=datetime.now()\n",
    "#launch_forward(PROJECT,'EVENT1','forward',modelfile,'REF_SEM')\n",
    "print (datetime.now()-start)\n",
    "launch_forward(PROJECT,'EVENT2','forward',modelfile,'REF_SEM')\n",
    "launch_forward(PROJECT,'EVENT3','forward',modelfile,'REF_SEM')\n",
    "launch_forward(PROJECT,'EVENT4','forward',modelfile,'REF_SEM')\n",
    "launch_forward(PROJECT,'EVENT5','forward',modelfile,'REF_SEM')\n",
    "launch_forward(PROJECT,'EVENT6','forward',modelfile,'REF_SEM')\n",
    "launch_forward(PROJECT,'EVENT7','forward',modelfile,'REF_SEM')\n",
    "launch_forward(PROJECT,'EVENT8','forward',modelfile,'REF_SEM')\n",
    "launch_forward(PROJECT,'EVENT9','forward',modelfile,'REF_SEM')\n",
    "launch_forward(PROJECT,'EVENT10','forward',modelfile,'REF_SEM')\n",
    "launch_forward(PROJECT,'EVENT11','forward',modelfile,'REF_SEM')\n",
    "launch_forward(PROJECT,'EVENT12','forward',modelfile,'REF_SEM')\n",
    "launch_forward(PROJECT,'EVENT13','forward',modelfile,'REF_SEM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfile=PROJECT+'MODELS/profile_syn.xyz'\n",
    "from datetime import datetime\n",
    "start=datetime.now()\n",
    "launch_forward(PROJECT,'EVENT1','forward',modelfile,'REF_SEM_SYN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print (datetime.now()-start)\n",
    "launch_forward(PROJECT,'EVENT2','forward',modelfile,'REF_SEM_SYN')\n",
    "launch_forward(PROJECT,'EVENT3','forward',modelfile,'REF_SEM_SYN')\n",
    "launch_forward(PROJECT,'EVENT4','forward',modelfile,'REF_SEM_SYN')\n",
    "launch_forward(PROJECT,'EVENT5','forward',modelfile,'REF_SEM_SYN')\n",
    "launch_forward(PROJECT,'EVENT6','forward',modelfile,'REF_SEM_SYN')\n",
    "launch_forward(PROJECT,'EVENT7','forward',modelfile,'REF_SEM_SYN')\n",
    "launch_forward(PROJECT,'EVENT8','forward',modelfile,'REF_SEM_SYN')\n",
    "launch_forward(PROJECT,'EVENT9','forward',modelfile,'REF_SEM_SYN')\n",
    "launch_forward(PROJECT,'EVENT10','forward',modelfile,'REF_SEM_SYN')\n",
    "launch_forward(PROJECT,'EVENT11','forward',modelfile,'REF_SEM_SYN')\n",
    "launch_forward(PROJECT,'EVENT12','forward',modelfile,'REF_SEM_SYN')\n",
    "launch_forward(PROJECT,'EVENT13','forward',modelfile,'REF_SEM_SYN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_source(dir_source):\n",
    "    file1 = open(dir_source, 'r')\n",
    "    Lines = file1.readlines()\n",
    "    x=np.float(Lines[2].split('=')[1].split('#')[0])\n",
    "    z=np.float(Lines[3].split('=')[1].split('#')[0])\n",
    "    return x,z\n",
    "def read_receiver(dir_receiver):\n",
    "    import re\n",
    "    file1 = open(dir_receiver, 'r')\n",
    "    Lines = file1.readlines()\n",
    "    number=len(Lines)\n",
    "    receiver_station=[]\n",
    "    receiver_net=[]\n",
    "    receiver_x=np.zeros(number)\n",
    "    receiver_z=np.zeros(number)\n",
    "    for _i, line in enumerate(Lines):\n",
    "        station,network,x,z,nun,nun=re.findall(r'\\S+', line)\n",
    "        #print(station)\n",
    "        receiver_station.append(station)\n",
    "        receiver_net.append(network)\n",
    "        receiver_x[_i]=np.float(x)\n",
    "        receiver_z[_i]=np.float(z)\n",
    "    return receiver_station,receiver_net,receiver_x,receiver_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adj_calculate(dir_run,dir_event,dir_obs,dir_syn,adj_src_type,stationpath):\n",
    "    import obspy\n",
    "    import pyadjoint\n",
    "    import numpy as np\n",
    "\n",
    "    from obspy.taup import TauPyModel\n",
    "    from obspy.core.trace import Trace\n",
    "    ##guarantee to change back to the project directory\n",
    "    os.chdir(dir_run)\n",
    "    os.chdir(dir_event)\n",
    "    ##### read source locations\n",
    "    source_x,source_depth_in_m=read_source('DATA/SOURCE')\n",
    "    ##### read receiver locations\n",
    "    receiver_station,receiver_net,receiver_x,receiver_z=read_receiver(stationpath)\n",
    "    import toml\n",
    "    Misfits_all=0.0\n",
    "    fh = open(dir_syn+'misfit.txt','w')\n",
    "    for i, station_name in enumerate(receiver_station):\n",
    "        #print(i)\n",
    "        #print(np.int(i))\n",
    "        distance=np.abs(receiver_x[i]-source_x)/111.0/1000.0\n",
    "        model = TauPyModel(model=\"iasp91\")\n",
    "        # adapted \n",
    "        arrivals = model.get_travel_times(source_depth_in_km=-source_depth_in_m/1000,distance_in_degree=distance) \n",
    "        time=arrivals[0].time\n",
    "        if time > 400-15:\n",
    "            window=[]\n",
    "        else:\n",
    "            window=[[time-5,time+15]]\n",
    "        print(window)\n",
    "        network=receiver_net[i]\n",
    "        station=receiver_station[i]\n",
    "        data_hetero=np.loadtxt(dir_syn+network+'.'+station+'.BXZ'+'.semd')\n",
    "        data_hetero_new=data_hetero.swapaxes(0,1)\n",
    "    #from obspy.core.trace import Trace\n",
    "        tr=Trace(data=data_hetero_new[1])\n",
    "        tr.stats.delta=0.02  ###here should be adapted to the DATA/Par_file\n",
    "        tr.stats.network = network\n",
    "        tr.stats.station = station\n",
    "        tr.stats.channel = 'BXZ'\n",
    "        data_hetero_obs=np.loadtxt(dir_obs+network+'.'+station+'.BXZ'+'.semd')                           \n",
    "        data_hetero_new_obs=data_hetero_obs.swapaxes(0,1)\n",
    "        tr_obs=Trace(data=data_hetero_new_obs[1])\n",
    "        tr_obs.stats.delta=0.02\n",
    "        tr_obs.stats.network = network\n",
    "        tr_obs.stats.station = station\n",
    "        tr_obs.stats.channel = 'BXZ'\n",
    "        ###here input a minum and maximum period\n",
    "        configure=pyadjoint.config.Config(10,120,measure_type='dt')\n",
    "        print(configure)\n",
    "        adj=pyadjoint.calculate_adjoint_source(\n",
    "        adj_src_type=adj_src_type, observed=tr_obs, synthetic=tr,\n",
    "        config=configure, window=window, plot=True);\n",
    "        Misfits_all+=adj.misfit\n",
    "        #fh.write(network+ ' ' + station+ ' '+ ' ' + str(adj.misfit))\n",
    "        #toml.dump(adj.misfit, fh)\n",
    "        adj.write(filename='SEM/'+network+'.'+station+'.BXZ'+'.adj',format=\"SPECFEM\", time_offset=-12)\n",
    "        adj=pyadjoint.calculate_adjoint_source(\n",
    "        adj_src_type=adj_src_type, observed=tr_obs, synthetic=tr, min_period=5, max_period=100,\n",
    "        config=configure, window=[], plot=True);\n",
    "        adj.write(filename='SEM/'+network+'.'+station+'.BXY'+'.adj',format=\"SPECFEM\", time_offset=-12)\n",
    "        \n",
    "        \n",
    "        data_hetero=np.loadtxt(dir_syn+network+'.'+station+'.BXX'+'.semd')\n",
    "        data_hetero_new=data_hetero.swapaxes(0,1)\n",
    "        tr=Trace(data=data_hetero_new[1])\n",
    "        tr.stats.delta=0.02\n",
    "        tr.stats.network = network\n",
    "        tr.stats.station = station\n",
    "        tr.stats.channel = 'BXX'\n",
    "        data_hetero_obs=np.loadtxt(dir_obs+network+'.'+station+'.BXX' +'.semd')                           \n",
    "        data_hetero_new_obs=data_hetero_obs.swapaxes(0,1)\n",
    "        tr_obs=Trace(data=data_hetero_new_obs[1])\n",
    "        tr_obs.stats.delta=0.02\n",
    "        tr_obs.stats.network = network\n",
    "        tr_obs.stats.station = station\n",
    "        tr_obs.stats.channel = 'BXX'\n",
    "        adj=pyadjoint.calculate_adjoint_source(\n",
    "        adj_src_type=adj_src_type, observed=tr_obs, synthetic=tr, min_period=5, max_period=100,\n",
    "        config=configure, window=window, plot=True);\n",
    "        #print(adj.misfit)\n",
    "        adj.write(filename='SEM/'+network+'.'+station+'.BXX'+'.adj',format=\"SPECFEM\", time_offset=-12)\n",
    "    fh.write(dir_syn+' ' + dir_event +' ' + str(Misfits_all)+\"\\n\")\n",
    "    fh.close()\n",
    "    os.chdir(dir_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_adjoint(dir_run,dir_event,dir_storesyn):\n",
    "    from datetime import datetime\n",
    "    os.chdir(dir_run)\n",
    "    os.chdir(dir_event)\n",
    "    from shutil import copyfile\n",
    "    import shutil\n",
    "    from distutils.dir_util import copy_tree\n",
    "    #copyfile(modelfile,'DATA/profile.xyz')\n",
    "    #os.remove('change_simulation_type.pl')\n",
    "    #s.system('ln -s /home/yjgao/install/specfem2d-devel/utils/change_simulation_type.pl')\n",
    "    #s.system('perl change_simulation_type.pl -f') \n",
    "    copyfile('../run_this_example_kernel.sh','run_this_example_kernel.sh')\n",
    "    print('start adjoint modelling',dir_event)\n",
    "    os.system('bash run_this_example_kernel.sh')\n",
    "    print('finished adjoint modelling')\n",
    "    ###store the synthetics to the dir observed or syn for backup\n",
    "    #!cat OUTPUT_FILES/proc000*_rho_kappa_mu_kernel.dat > OUTPUT_FILES/rho_kappa_mu_kernel.dat\n",
    "    ####prepared to \n",
    "    os.system('cat OUTPUT_FILES/proc000*_rhop_alpha_beta_kernel.dat > OUTPUT_FILES/rho_alpha_beta_kernel.dat')\n",
    "    shutil.copy('OUTPUT_FILES/rho_alpha_beta_kernel.dat', dir_storesyn)\n",
    "    os.chdir(dir_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_events=['EVENT1','EVENT2','EVENT3','EVENT4','EVENT5','EVENT6','EVENT7','EVENT8','EVENT9','EVENT10','EVENT11','EVENT12','EVENT13']\n",
    "for _i, event in enumerate(list_events):\n",
    "    adj_calculate(PROJECT,event,'REF_SEM/','REF_SEM_SYN/','ccc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_events=['EVENT1','EVENT2','EVENT3','EVENT4','EVENT5','EVENT6','EVENT7','EVENT8','EVENT9','EVENT10','EVENT11','EVENT12','EVENT13']\n",
    "for _i, event in enumerate(list_events):\n",
    "    launch_adjoint(PROJECT,event,'REF_SEM_SYN/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_events=['EVENT1','EVENT2','EVENT3','EVENT4','EVENT5','EVENT6','EVENT7','EVENT8','EVENT9','EVENT10','EVENT11','EVENT12','EVENT13']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os.path\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "except:\n",
    "    print(\"Error importing pyplot from matplotlib, please install matplotlib package first...\")\n",
    "    sys.tracebacklimit=0\n",
    "    raise Exception(\"Importing matplotlib failed\")\n",
    "###controlling the speed for smoothing!\n",
    "def grid(x, y, z, resX=500, resY=500):\n",
    "    \"\"\"\n",
    "    Converts 3 column data to matplotlib grid\n",
    "    \"\"\"\n",
    "    from scipy.interpolate import griddata\n",
    "    #from scipy.interpolate import griddata\n",
    "\n",
    "    xi = np.linspace(min(x), max(x), resX)\n",
    "    yi = np.linspace(max(y), min(y), resY)\n",
    "\n",
    "    # mlab version\n",
    "    Z = griddata((x, y), z, (xi[None,:], yi[:,None]), method='linear')\n",
    "    # scipy version\n",
    "    #Z = griddata((x, y), z, (xi[None,:], yi[:,None]), method='cubic')\n",
    "\n",
    "    X, Y = np.meshgrid(xi, yi)\n",
    "    return X, Y, Z\n",
    "\n",
    "def plot_kernels(filename,show=False,EVENT='EVENT',total_max=1e-10):\n",
    "    \"\"\"\n",
    "    plots ASCII kernel file\n",
    "    \"\"\"\n",
    "    #print \"plotting kernel file: \",filename\n",
    "    #print \"\"\n",
    "\n",
    "    data = np.loadtxt(filename)\n",
    "\n",
    "    # checks data\n",
    "    if data.ndim != 2:\n",
    "        #print \"Error: wrong data dimension for kernel file\",data.ndim\n",
    "        sys.tracebacklimit=0\n",
    "        raise Exception(\"Invalid data dimension\")\n",
    "\n",
    "    # checks array\n",
    "    if len(data[1,:]) != 5:\n",
    "        #print \"data shape  : \",data.shape\n",
    "        #print \"data lengths: \",len(data[:,1]),len(data[1,:])\n",
    "        #print \"Error: wrong data format for kernel file\",data.shape\n",
    "        sys.tracebacklimit=0\n",
    "        raise Exception(\"Invalid data format\")\n",
    "\n",
    "    # splits up data\n",
    "    x = data[:,0]\n",
    "    y = data[:,1]\n",
    "\n",
    "    #print \"dimensions:\"\n",
    "    #print \"  x-range min/max = %f / %f\" % (x.min(), x.max())\n",
    "    #print \"  y-range min/max = %f / %f\" % (y.min(), y.max())\n",
    "    #print \"\"\n",
    "\n",
    "    z1 = data[:,2] # e.g. rho\n",
    "    z2 = data[:,3] # e.g. alpha\n",
    "    z3 = data[:,4] # e.g. beta\n",
    "\n",
    "    # names like\n",
    "    #   rhop_alpha_beta_kernel.dat\n",
    "    # or\n",
    "    #   proc000000_rhop_alpha_beta_kernel.dat\n",
    "    name = os.path.basename(filename)\n",
    "\n",
    "    name_kernels = str.split(name,\"_\")\n",
    "    if len(name_kernels) == 4:\n",
    "        kernel1 = 'K_' + name_kernels[0] # rhop\n",
    "        kernel2 = 'K_' + name_kernels[1] # alpha\n",
    "        kernel3 = 'K_' + name_kernels[2] # beta\n",
    "    elif len(name_kernels) == 5:\n",
    "        kernel1 = 'K_' + name_kernels[1]\n",
    "        kernel2 = 'K_' + name_kernels[2]\n",
    "        kernel3 = 'K_' + name_kernels[3]\n",
    "    else:\n",
    "        kernel1 = 'K_1'\n",
    "        kernel2 = 'K_2'\n",
    "        kernel3 = 'K_3'\n",
    "\n",
    "    #print \"statistics:\"\n",
    "    #print \"  %12s : min/max = %e / %e\" % (kernel1,z1.min(),z1.max())\n",
    "    #print \"  %12s : min/max = %e / %e\" % (kernel2,z2.min(),z2.max())\n",
    "    #print \"  %12s : min/max = %e / %e\" % (kernel3,z3.min(),z3.max())\n",
    "    #print \"\"\n",
    "\n",
    "    #total_max = 0.1*abs(z2).max()\n",
    "    print(total_max)\n",
    "    #print \"  data max = \",total_max\n",
    "    #print \"\"\n",
    "    #print(total_max)\n",
    "    #total_max = 1.e-8\n",
    "\n",
    "    # setup figure (with 3 subplots)\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=1)\n",
    "\n",
    "    for i,ax in enumerate(axes.flat,start=1):\n",
    "        # top\n",
    "        if i == 1:\n",
    "            X, Y, Z = grid(x,y,z1)\n",
    "            ax.set_title(\"Kernels\")\n",
    "            ax.set_ylabel('RHO')\n",
    "        elif i == 2:\n",
    "            X, Y, Z = grid(x,y,z2)\n",
    "            ax.set_ylabel('VP')\n",
    "        elif i == 3:\n",
    "            X, Y, Z = grid(x,y,z3)\n",
    "            ax.set_ylabel('VS')\n",
    "\n",
    "        #colormap = 'jet'\n",
    "        colormap = 'RdBu'\n",
    "\n",
    "        im = ax.imshow(Z,vmax=total_max, vmin=-total_max,extent=[x.min(), x.max(), y.min(), y.max()],cmap=colormap)\n",
    "        #ax.set_xlim(2000000,3000000)\n",
    "        #ax.set_ylim(-200000,0)\n",
    "    # moves plots together\n",
    "    fig.subplots_adjust(hspace=0)\n",
    "    plt.setp([a.get_xticklabels() for a in fig.axes[:-1]], visible=False)\n",
    "    \n",
    "    # colorbar\n",
    "    fig.colorbar(im, ax=axes.ravel().tolist())\n",
    "    #plt.xlim(2500000,3000000)\n",
    "    #plt.ylim(-200000,0)\n",
    "    #fig.colorbar(im, ax=axes.ravel().tolist(),orientation='horizontal')\n",
    "\n",
    "    # show the figure\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "    # saves kernel figure as file\n",
    "    dir = os.path.dirname('.')\n",
    "    name_without_ending = str.split(name,\".\")[0]\n",
    "    outfile = dir + \"/\" + name_without_ending + \".png\"\n",
    "    fig.savefig(EVENT+'_test_ccc.png', format=\"png\",  dpi=300)\n",
    "\n",
    "    #print \"*****\"\n",
    "    #print \"plotted file: \",outfile\n",
    "    #print \"*****\"\n",
    "    #print \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_models(filename,show=False,EVENT='EVENT',total_max=10000,total_min=5000):\n",
    "    \"\"\"\n",
    "    plots ASCII kernel file\n",
    "    \"\"\"\n",
    "    #print \"plotting kernel file: \",filename\n",
    "    #print \"\"\n",
    "\n",
    "    data = np.loadtxt(filename)\n",
    "\n",
    "    # checks data\n",
    "    if data.ndim != 2:\n",
    "        #print \"Error: wrong data dimension for kernel file\",data.ndim\n",
    "        sys.tracebacklimit=0\n",
    "        raise Exception(\"Invalid data dimension\")\n",
    "\n",
    "    # checks array\n",
    "    if len(data[1,:]) != 5:\n",
    "        #print \"data shape  : \",data.shape\n",
    "        #print \"data lengths: \",len(data[:,1]),len(data[1,:])\n",
    "        #print \"Error: wrong data format for kernel file\",data.shape\n",
    "        sys.tracebacklimit=0\n",
    "        raise Exception(\"Invalid data format\")\n",
    "\n",
    "    # splits up data\n",
    "    x = data[:,0]\n",
    "    y = data[:,1]\n",
    "\n",
    "    #print \"dimensions:\"\n",
    "    #print \"  x-range min/max = %f / %f\" % (x.min(), x.max())\n",
    "    #print \"  y-range min/max = %f / %f\" % (y.min(), y.max())\n",
    "    #print \"\"\n",
    "\n",
    "    z1 = data[:,2] # e.g. rho\n",
    "    z2 = data[:,3] # e.g. alpha\n",
    "    z3 = data[:,4] # e.g. beta\n",
    "\n",
    "    # names like\n",
    "    #   rhop_alpha_beta_kernel.dat\n",
    "    # or\n",
    "    #   proc000000_rhop_alpha_beta_kernel.dat\n",
    "    name = os.path.basename(filename)\n",
    "\n",
    "    name_kernels = str.split(name,\"_\")\n",
    "    if len(name_kernels) == 4:\n",
    "        kernel1 = 'K_' + name_kernels[0] # rhop\n",
    "        kernel2 = 'K_' + name_kernels[1] # alpha\n",
    "        kernel3 = 'K_' + name_kernels[2] # beta\n",
    "    elif len(name_kernels) == 5:\n",
    "        kernel1 = 'K_' + name_kernels[1]\n",
    "        kernel2 = 'K_' + name_kernels[2]\n",
    "        kernel3 = 'K_' + name_kernels[3]\n",
    "    else:\n",
    "        kernel1 = 'K_1'\n",
    "        kernel2 = 'K_2'\n",
    "        kernel3 = 'K_3'\n",
    "\n",
    "    #print \"statistics:\"\n",
    "    #print \"  %12s : min/max = %e / %e\" % (kernel1,z1.min(),z1.max())\n",
    "    #print \"  %12s : min/max = %e / %e\" % (kernel2,z2.min(),z2.max())\n",
    "    #print \"  %12s : min/max = %e / %e\" % (kernel3,z3.min(),z3.max())\n",
    "    #print \"\"\n",
    "\n",
    "    #total_max = 0.1*abs(z2).max()\n",
    "    print(total_max)\n",
    "    #print \"  data max = \",total_max\n",
    "    #print \"\"\n",
    "    #print(total_max)\n",
    "    #total_max = 1.e-8\n",
    "\n",
    "    # setup figure (with 3 subplots)\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=1)\n",
    "\n",
    "    for i,ax in enumerate(axes.flat,start=1):\n",
    "        # top\n",
    "        if i == 1:\n",
    "            X, Y, Z = grid(x,y,z1)\n",
    "            ax.set_title(\"Kernels\")\n",
    "            ax.set_ylabel('VP')\n",
    "        elif i == 2:\n",
    "            X, Y, Z = grid(x,y,z2)\n",
    "            ax.set_ylabel('VS')\n",
    "        elif i == 3:\n",
    "            X, Y, Z = grid(x,y,z3)\n",
    "            ax.set_ylabel('RHO')\n",
    "\n",
    "        #colormap = 'jet'\n",
    "        colormap = plt.cm.get_cmap('jet')\n",
    "\n",
    "        im = ax.imshow(Z,vmax=total_max, vmin=total_min,extent=[x.min(), x.max(), y.min(), y.max()],cmap=colormap.reversed())\n",
    "        #ax.set_xlim(2000000,3000000)\n",
    "        #ax.set_ylim(-200000,0)\n",
    "    # moves plots together\n",
    "    fig.subplots_adjust(hspace=0)\n",
    "    plt.setp([a.get_xticklabels() for a in fig.axes[:-1]], visible=False)\n",
    "    \n",
    "    # colorbar\n",
    "    fig.colorbar(im, ax=axes.ravel().tolist())\n",
    "    #plt.xlim(2500000,3000000)\n",
    "    #plt.ylim(-200000,0)\n",
    "    #fig.colorbar(im, ax=axes.ravel().tolist(),orientation='horizontal')\n",
    "\n",
    "    # show the figure\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "    # saves kernel figure as file\n",
    "    dir = os.path.dirname('.')\n",
    "    name_without_ending = str.split(name,\".\")[0]\n",
    "    outfile = dir + \"/\" + name_without_ending + \".png\"\n",
    "    fig.savefig(EVENT+'_test_ccc.png', format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"EVENT2/REF_SEM_SYN/rho_alpha_beta_kernel.dat\"\n",
    "#!cp OUTPUT_FILES/rho_alpha_beta_kernel.dat OUTPUT_FILES_bk/rho_alpha_beta_kernel_cc.dat \n",
    "plot_kernels(filename,show=True,EVENT='EVENT2')\n",
    "\n",
    "filename=\"EVENT1/REF_SEM_SYN/rho_alpha_beta_kernel.dat\"\n",
    "#!cp OUTPUT_FILES/rho_alpha_beta_kernel.dat OUTPUT_FILES_bk/rho_alpha_beta_kernel_cc.dat \n",
    "plot_kernels(filename,show=True ,EVENT='EVENT1')\n",
    "\n",
    "filename=\"EVENT3/REF_SEM_SYN/rho_alpha_beta_kernel.dat\"\n",
    "#!cp OUTPUT_FILES/rho_alpha_beta_kernel.dat OUTPUT_FILES_bk/rho_alpha_beta_kernel_cc.dat \n",
    "plot_kernels(filename,show=True,EVENT='EVENT3')\n",
    "\n",
    "filename=\"EVENT4/REF_SEM_SYN/rho_alpha_beta_kernel.dat\"\n",
    "#!cp OUTPUT_FILES/rho_alpha_beta_kernel.dat OUTPUT_FILES_bk/rho_alpha_beta_kernel_cc.dat \n",
    "plot_kernels(filename,show=True,EVENT='EVENT4')\n",
    "\n",
    "filename=\"EVENT5/REF_SEM_SYN/rho_alpha_beta_kernel.dat\"\n",
    "#!cp OUTPUT_FILES/rho_alpha_beta_kernel.dat OUTPUT_FILES_bk/rho_alpha_beta_kernel_cc.dat \n",
    "plot_kernels(filename,show=True,EVENT='EVENT5')\n",
    "\n",
    "filename=\"EVENT6/REF_SEM_SYN/rho_alpha_beta_kernel.dat\"\n",
    "#!cp OUTPUT_FILES/rho_alpha_beta_kernel.dat OUTPUT_FILES_bk/rho_alpha_beta_kernel_cc.dat \n",
    "plot_kernels(filename,show=True,EVENT='EVENT6')\n",
    "\n",
    "filename=\"EVENT7/REF_SEM_SYN/rho_alpha_beta_kernel.dat\"\n",
    "#!cp OUTPUT_FILES/rho_alpha_beta_kernel.dat OUTPUT_FILES_bk/rho_alpha_beta_kernel_cc.dat \n",
    "plot_kernels(filename,show=True,EVENT='EVENT7')\n",
    "\n",
    "filename=\"EVENT8/REF_SEM_SYN/rho_alpha_beta_kernel.dat\"\n",
    "#!cp OUTPUT_FILES/rho_alpha_beta_kernel.dat OUTPUT_FILES_bk/rho_alpha_beta_kernel_cc.dat \n",
    "plot_kernels(filename,show=True,EVENT='EVENT8')\n",
    "\n",
    "filename=\"EVENT9/REF_SEM_SYN/rho_alpha_beta_kernel.dat\"\n",
    "#!cp OUTPUT_FILES/rho_alpha_beta_kernel.dat OUTPUT_FILES_bk/rho_alpha_beta_kernel_cc.dat \n",
    "plot_kernels(filename,show=True,EVENT='EVENT9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"EVENT7/REF_SEM_SYN/rho_alpha_beta_kernel.dat\"\n",
    "#!cp OUTPUT_FILES/rho_alpha_beta_kernel.dat OUTPUT_FILES_bk/rho_alpha_beta_kernel_cc.dat \n",
    "plot_kernels(filename,show=True,EVENT='EVENT7',total_max=2e-10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"EVENT7/REF_SEM_SYN/rho_alpha_beta_kernelsmsrc.dat\"\n",
    "plot_kernels(filename,show=True,EVENT='EVENT7_smoothsource',total_max=2e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"EVENT4/REF_SEM_SYN/rho_alpha_beta_kernelsmsrc.dat\"\n",
    "plot_kernels(filename,show=True,EVENT='EVENT4_smoothsource',total_max=2e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"EVENT7/REF_SEM_SYN/rho_alpha_beta_kernelsdist.dat\"\n",
    "plot_kernels(filename,show=True,EVENT='EVENT7_smoothsource_weight',total_max=1,total_min=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_gradient(dir_gradient):\n",
    "    gradient=np.loadtxt(dir_gradient)\n",
    "    x=gradient.T[0]\n",
    "    z=gradient.T[1]\n",
    "    rho=gradient.T[2]\n",
    "    alpha=gradient.T[3]\n",
    "    belta=gradient.T[4]\n",
    "    return x,z,rho,alpha,belta\n",
    "def read_modelfile(modelfile):\n",
    "    model=np.loadtxt('MODELS/'+modelfile)\n",
    "    x_new=model.T[0]\n",
    "    z_new=model.T[1]\n",
    "    vp=model.T[2]\n",
    "    vs=model.T[3]\n",
    "    rho=model.T[4]\n",
    "    return x_new,z_new,vp,vs,rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new,z_new,vp,vs,rho=read_gradient('EVENT1'+'/REF_SEM_SYN/rho_alpha_beta_kernel.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_gradient(list_events,iteration,smoothsrc='True'):\n",
    "    if smoothsrc=='True':\n",
    "        gradient=np.loadtxt(list_events[0]+'/REF_SEM_'+iteration+'/'+'rho_alpha_beta_kernelsmsrc.dat')\n",
    "        pendix='rho_alpha_beta_kernelsmsrc.dat'\n",
    "    else:\n",
    "        gradient=np.loadtxt(list_events[0]+'/REF_SEM_'+iteration+'/'+'rho_alpha_beta_kernel.dat')\n",
    "        pendix='rho_alpha_beta_kernel.dat'\n",
    "    x_new,z_new,vp,vs,rho=read_gradient(list_events[0]+'/REF_SEM_'+iteration+'/'+'rho_alpha_beta_kernelsmsrc.dat')\n",
    "    gradient_sum=np.zeros((x.shape))\n",
    "    for _i,gradient in enumerate(list_events):\n",
    "        x,z,rho,alpha,belta=read_gradient(list_events[_i]+'/REF_SEM_'+iteration+'/'+pendix)\n",
    "        gradient_sum.T[2]+=rho\n",
    "        gradient_sum.T[3]+=alpha\n",
    "        gradient_sum.T[4]+=belta\n",
    "    gradient_sum.T[0]=x\n",
    "    gradient_sum.T[1]=z\n",
    "    np.savetxt(iteration+'_summed_gradient.dat',gradient_sum)\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_source_region_from_gradient_gauss(EVENT:str,iteration:str, radius_to_cut: float):\n",
    "    \"\"\"\n",
    "    Sources often show unreasonable sensitivities. This function\n",
    "    brings the value of the gradient down to zero for that region.\n",
    "    I recommend doing this before smoothing.\n",
    "    \n",
    "    :param mesh: Path to the mesh\n",
    "    :type mesh: str\n",
    "    :param source_location: Source latitude, longitude and depth\n",
    "    :type source_location: dict\n",
    "    :param radius_to_cut: Radius to cut in km\n",
    "    :type radius_to_cut: float\n",
    "    \"\"\"\n",
    "    import lasif\n",
    "    dir_gradient= EVENT+ '/REF_SEM_' +iteration+'/rho_alpha_beta_kernel.dat'\n",
    "    #print(mesh)\n",
    "    x,z,rho,alpha,belta=read_gradient(dir_gradient)\n",
    "    gradient=np.loadtxt(dir_gradient)\n",
    "    s_x,s_z=read_source(EVENT+'/DATA/SOURCE')\n",
    "    dist = np.sqrt( (x - s_x) ** 2 + (z - s_z) ** 2)\n",
    "    \n",
    "    ####here we implement 1-Gaussian smoothing the source\n",
    "    #distance_smooth = (x[k] * np.ones(number_nodes) - x) ** 2 + (y[k] * np.ones(number_nodes) - y)** 2 + (z[k] * np.ones(number_nodes) - z) ** 2\n",
    "    distance_smooth1 = np.exp(-dist/ (2 * radius_to_cut ** 2))\n",
    "    print(dist.max(),distance_smooth1.max())\n",
    "    distance_smooth1 = distance_smooth1 / distance_smooth1.max()\n",
    "    print(distance_smooth1.max())\n",
    "    alpha_new=alpha*(1-distance_smooth1)\n",
    "    belta_new=belta*(1-distance_smooth1)\n",
    "    rho_new=rho*(1-distance_smooth1)\n",
    "    gradient_new=gradient.copy()\n",
    "    smooth_dist=gradient.copy()\n",
    "    gradient_new.T[2]=rho_new\n",
    "    gradient_new.T[3]=alpha_new\n",
    "    gradient_new.T[4]=belta_new\n",
    "    smooth_dist.T[2]=1-distance_smooth1\n",
    "    smooth_dist.T[3]=1-distance_smooth1\n",
    "    smooth_dist.T[4]=1-distance_smooth1\n",
    "    np.savetxt(EVENT+ '/REF_SEM_' +iteration+'/rho_alpha_beta_kernelsmsrc.dat',gradient_new)\n",
    "    np.savetxt(EVENT+ '/REF_SEM_' +iteration+'/rho_alpha_beta_kernelsdist.dat',smooth_dist)\n",
    "    \n",
    "\n",
    "\n",
    "def smooth_cupy_new(xyz_matrix,iterations,sigmaxy,sigmaz,Velocity_matrix): \n",
    "      ##input a numpy array and using asarray move to the gpu device\n",
    "      import cupy as cp\n",
    "      Velocity_matrix=cp.asarray(Velocity_matrix)\n",
    "      xyz_matrix=cp.asarray(xyz_matrix)\n",
    "      Vpv_smooth_all=cp.zeros((3,iterations),dtype=float)  \n",
    "      #t1 = datetime.datetime.now()       \n",
    "      for i in range(iterations):  \n",
    "            distancex=(cp.square(xyz_matrix[0][i]-xyz_matrix[0]))/(2*sigmaxy*sigmaxy)\n",
    "            distancey=(cp.square(xyz_matrix[1][i]-xyz_matrix[1]))/(2*sigmaxy*sigmaxy)\n",
    "            #distancez=(cp.square(xyz_matrix[2][i]-xyz_matrix[2]))/(2*sigmaz*sigmaz)\n",
    "            #print(distance)\n",
    "            distance=distancex+distancey\n",
    "            distance=cp.exp(-distance)\n",
    "            distance= cp.divide(distance,cp.sum(distance))\n",
    "            #print(distance)\n",
    "            Vpv_smooth_all[0][i] = cp.dot(distance, Velocity_matrix[0])\n",
    "            Vpv_smooth_all[1][i] = cp.dot(distance, Velocity_matrix[1])\n",
    "            Vpv_smooth_all[2][i] = cp.dot(distance, Velocity_matrix[2])\n",
    "      #print(t2-t1)\n",
    "      # move array from gpu device to the host\n",
    "      return cp.asnumpy(Vpv_smooth_all)\n",
    "\n",
    "def smooth_numpy_new(xyz_matrix,iterations,sigmaxy,sigmaz,Velocity_matrix): \n",
    "      ##input a numpy array and using asarray move to the gpu device\n",
    "     # import cupy as cp\n",
    "      Velocity_matrix=np.asarray(Velocity_matrix)\n",
    "      xyz_matrix=np.asarray(xyz_matrix)\n",
    "      Vpv_smooth_all=np.zeros((3,iterations),dtype=float)  \n",
    "      #t1 = datetime.datetime.now()       \n",
    "      for i in range(iterations):  \n",
    "            distancex=(np.square(xyz_matrix[0][i]-xyz_matrix[0]))/(2*sigmaxy*sigmaxy)\n",
    "            distancey=(np.square(xyz_matrix[1][i]-xyz_matrix[1]))/(2*sigmaxy*sigmaxy)\n",
    "            #distancez=(cp.square(xyz_matrix[2][i]-xyz_matrix[2]))/(2*sigmaz*sigmaz)\n",
    "            #print(distance)\n",
    "            distance=distancex+distancey\n",
    "            distance=np.exp(-distance)\n",
    "            distance= np.divide(distance,np.sum(distance))\n",
    "            #print(distance)\n",
    "            Vpv_smooth_all[0][i] = np.dot(distance, Velocity_matrix[0])\n",
    "            Vpv_smooth_all[1][i] = np.dot(distance, Velocity_matrix[1])\n",
    "            Vpv_smooth_all[2][i] = np.dot(distance, Velocity_matrix[2])\n",
    "      #print(t2-t1)\n",
    "      # move array from gpu device to the host\n",
    "      return Vpv_smooth_all\n",
    "\n",
    "\n",
    "def smooth_compat(x,y,Vp,Vs,rho,sigmaxy,sigmaz):   \n",
    "    number_nodes=x.shape[0]\n",
    "    print(number_nodes)\n",
    "    V_smooth_all=np.zeros((3,number_nodes),dtype=float)\n",
    "    xyz_matrix=np.zeros((2,number_nodes),dtype=float)\n",
    "    xyz_matrix[0]=x\n",
    "    xyz_matrix[1]=y\n",
    "    Velocity_matrix=np.zeros((3,number_nodes),dtype=float)\n",
    "    Velocity_matrix[0]=Vp\n",
    "    Velocity_matrix[1]=Vs\n",
    "    Velocity_matrix[2]=rho\n",
    "    iterations=number_nodes\n",
    "    #Vpv1_smooth_all=np.zeros((5,206313),dtype=float)\n",
    "    test_cupy=smooth_cupy_new(xyz_matrix,iterations,sigmaxy,sigmaz,Velocity_matrix)\n",
    "    return test_cupy\n",
    "\n",
    "def smooth_kernel(gradient,iteration,sigmax,sigmaz):\n",
    "    x,z,rho_kernel,alpha_kernel,belta_kernel=read_gradient(gradient)\n",
    "    smooth_format=smooth_compat(x,z,alpha_kernel,belta_kernel,rho_kernel,sigmax,sigmaz)\n",
    "    smooth_kernel=np.zeros((5,x.shape[0]))\n",
    "    smooth_kernel[0]=x\n",
    "    smooth_kernel[1]=z\n",
    "    ####follow the sequence of specfem2d for the gradient field rho, p, s\n",
    "    smooth_kernel[2]=smooth_format[2]\n",
    "    smooth_kernel[3]=smooth_format[0]\n",
    "    smooth_kernel[4]=smooth_format[1]\n",
    "    np.savetxt(iteration+'_summed_gradient_smooth.dat',smooth_kernel.T)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_events=['EVENT1','EVENT2','EVENT3','EVENT4','EVENT5','EVENT6','EVENT7','EVENT8','EVENT9']\n",
    "iteration='SYN'\n",
    "for _i,event in enumerate(list_events):\n",
    "    cut_source_region_from_gradient_gauss(event,iteration, 600)\n",
    "sum_gradient(list_events,iteration, smoothsrc='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###smooth kernel \n",
    "iteration='SYN'\n",
    "###1 degree to smooth, you need to tune this somehow? depends on you\n",
    "smooth_kernel(iteration+'_summed_gradient.dat',iteration,30000,30000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration='SYN'\n",
    "plot_kernels(iteration+'_summed_gradient.dat',show=True,EVENT='summed_smooth',total_max=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration='SYN'\n",
    "plot_kernels(iteration+'_summed_gradient_smooth.dat',show=True,EVENT='summed_smooth',total_max=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iteration='SYN'\n",
    "plot_kernels(iteration+'_summed_gradient.dat',show=True,EVENT='summed_nosmooth',total_max=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_kernel_format(gradient,x_new,z_new):\n",
    "    x,z,rho_kernel,alpha_kernel,belta_kernel=read_gradient(gradient)\n",
    "    from scipy.interpolate import griddata\n",
    "    alpha_kernel_newformat=griddata((x,z),alpha_kernel,(x_new,z_new),method='cubic')\n",
    "    belta_kernel_newformat=griddata((x,z),belta_kernel,(x_new,z_new),method='cubic')\n",
    "    rho_kernel_newformat=griddata((x,z),rho_kernel,(x_new,z_new),method='cubic')\n",
    "###here because of the nan when using cubic at the boundary points\n",
    "    nan_index=np.argwhere(np.isnan(alpha_kernel_newformat))\n",
    "    alpha_kernel_newformat[nan_index]=griddata((x,z),alpha_kernel,(x_new[nan_index],z_new[nan_index]),method='nearest')\n",
    "    belta_kernel_newformat[nan_index]=griddata((x,z),belta_kernel,(x_new[nan_index],z_new[nan_index]),method='nearest')\n",
    "    rho_kernel_newformat[nan_index]  =griddata((x,z),rho_kernel,  (x_new[nan_index],z_new[nan_index]),method='nearest')\n",
    "    gradient_new=np.zeros((3,x_new.shape[0]))\n",
    "    gradient_new[2]=rho_kernel_newformat\n",
    "    gradient_new[0]=alpha_kernel_newformat\n",
    "    gradient_new[1]=belta_kernel_newformat\n",
    "    \n",
    "    return gradient_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_update_steepest(gradient,model,perturb_Vp,perturb_Vs,perturb_rho):\n",
    "    model=np.loadtxt(model)\n",
    "    x_new=model.T[0]\n",
    "    z_new=model.T[1]\n",
    "    vp=model.T[2]\n",
    "    vs=model.T[3]\n",
    "    rho=model.T[4]\n",
    "    ##through this, reforge the shape from rho, vp and vs to vp vs and rho, and interpolate\n",
    "    gradient=interpolate_kernel_format(gradient,x_new,z_new)\n",
    "    gradient_Vp_abs=np.absolute(gradient[0]) \n",
    "    gradient_Vs_abs=np.absolute(gradient[1]) \n",
    "    gradient_rho_abs=np.absolute(gradient[2]) \n",
    "\n",
    "    print('maxgrdientVp','maxgrdientVs','maxgrdientrho')\n",
    "    print(np.max(gradient_Vp_abs),np.max(gradient_Vs_abs),np.max(gradient_rho_abs))  \n",
    "    alpha_Vp=perturb_Vp*vp/gradient_Vp_abs \n",
    "    alpha_Vs=perturb_Vs*vs/gradient_Vs_abs \n",
    "    alpha_rho=perturb_rho*rho/gradient_rho_abs\n",
    "    step_Vp= np.min(alpha_Vp)\n",
    "    step_Vs= np.min(alpha_Vs)\n",
    "    step_rho= np.min(alpha_rho)\n",
    "    print('stepVp','stepVs','steprho')\n",
    "    print( step_Vp,step_Vs,step_rho)\n",
    "    print('variationmax for Vp,  Vs, Vsh,rho')\n",
    "    print( np.max(np.abs(step_Vp*gradient[0])),np.max(np.abs(step_Vs*gradient[1])),np.max(np.abs(step_rho*gradient[2])))\n",
    "    VpNew= model.T[2] - step_Vp*gradient[0]\n",
    "    VsNew= model.T[3] - step_Vs*gradient[1]\n",
    "    rhoNew= model.T[4] - step_rho*gradient[2]\n",
    "    return x_new,z_new,VpNew,VsNew,rhoNew\n",
    "\n",
    "def generate_newmodel(vp,vs,rho,x,z,newfile,newfile_noheader):  \n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    maxx=45*111*1000\n",
    "    minx=0\n",
    "    maxz=0\n",
    "    minz=-1000*1000\n",
    "    nx = 800 # Number of sampling points in x direction\n",
    "    nz = 200  # Number of sampling points in z direction\n",
    "    spacing_x = (maxx-minx)/(nx-1)\n",
    "    spacing_z = (maxz-minz)/(nz-1)\n",
    "    print (\"space x space z\", spacing_x,spacing_z)\n",
    "    fo = open(newfile, \"w+\") # Name of the file it has to be set in the Par_file\n",
    "    fo2 = open(newfile_noheader, \"w+\")\n",
    "    print( \"Name of the file: \", fo.name)\n",
    "    orig_x=0\n",
    "    orig_z=0\n",
    "    end_x=maxx\n",
    "    end_z=maxz\n",
    "    line1 = str(orig_x)+\" \"+str(orig_z)+\" \"+str(end_x)+\" \"+str(end_z)+\"\\n\"\n",
    "    line2 = str(spacing_x)+\" \"+str(spacing_z)+\"\\n\"\n",
    "    line3 = str(nx)+\" \"+str(nz)+\"\\n\"\n",
    "# line4 : vpMin vpMax vsMin vsMax rhoMin rhoMax\n",
    "    line4 = str(min(vp))+\" \"+str(max(vp))+\" \"+str(min(vs))+\" \"+str(max(vs))+\" \"+str(min(rho))+\" \"+str(max(rho))+\"\\n\"\n",
    "# Write a line at the end of the file.\n",
    "    fo.write(line1)\n",
    "    fo.write(line2)\n",
    "    fo.write(line3)\n",
    "    fo.write(line4)\n",
    "    m=x.shape[0]\n",
    "    for i in np.arange(0,m):\n",
    "        lineToWrite= str(x[i])+\" \"+str(z[i])+\" \"+str(vp[i])+\" \"+str(vs[i])+\" \"+str(rho[i])+\"\\n\"\n",
    "        fo.write(lineToWrite)\n",
    "        fo2.write(lineToWrite)\n",
    "    fo.close()\n",
    "    fo.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new,z_new,vpnew,vsnew,rhonew=model_update_steepest('SYN_summed_gradient_smooth.dat','MODELS/profile_syn_numpy.xyz',0.01,0.0001,0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_newmodel(vpnew,vsnew,rhonew,x_new,z_new,'MODELS/profile_syn_iteration2.xyz','MODELS/profile_syn_iteration2_noheader.xyz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_syn_iteration2_noheader.xyz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfile=PROJECT+'MODELS/profile_syn_iteration2.xyz'\n",
    "from datetime import datetime\n",
    "list_events=['EVENT1','EVENT2','EVENT3','EVENT4','EVENT5','EVENT6','EVENT7','EVENT8','EVENT9','EVENT10','EVENT11','EVENT12','EVENT13']\n",
    "for _i, event in enumerate(list_events):\n",
    "    start=datetime.now()\n",
    "    launch_forward(PROJECT,event,'forward',modelfile,'REF_SEM_SYN2')\n",
    "    print (datetime.now()-start, event )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_events=['EVENT1','EVENT2','EVENT3','EVENT4','EVENT5','EVENT6','EVENT7','EVENT8','EVENT9','EVENT10','EVENT11','EVENT12','EVENT13']\n",
    "for _i, event in enumerate(list_events):\n",
    "    adj_calculate(PROJECT,event,'REF_SEM/','REF_SEM_SYN2/','ccc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_misfit(list_events,ITERATION):\n",
    "    import re\n",
    "    ALL_misfit=0\n",
    "    for _i, event in enumerate(list_events):\n",
    "        print(event)\n",
    "        file1 = open(event+'/'+'REF_SEM_'+ITERATION+'/misfit.txt', 'r')\n",
    "        Lines = file1.readlines()\n",
    "        path,event,misfit=re.findall(r'\\S+', Lines[0])\n",
    "        print(misfit)\n",
    "        ALL_misfit+=float(misfit)\n",
    "    return ALL_misfit\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_events=['EVENT1','EVENT2','EVENT3','EVENT4','EVENT5','EVENT6','EVENT7','EVENT8','EVENT9','EVENT10','EVENT11','EVENT12','EVENT13']\n",
    "retrieve_misfit(list_events,'SYN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_misfit(list_events,'SYN2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(176-163.88)/176"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_events=['EVENT1','EVENT2','EVENT3','EVENT4','EVENT5','EVENT6','EVENT7','EVENT8','EVENT9','EVENT10','EVENT11','EVENT12','EVENT13']\n",
    "for _i, event in enumerate(list_events):\n",
    "    launch_adjoint(PROJECT,event,'REF_SEM_SYN2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_events=['EVENT1','EVENT2','EVENT3','EVENT4','EVENT5','EVENT6','EVENT7','EVENT8','EVENT9']\n",
    "iteration='SYN2'\n",
    "list_events=['EVENT1','EVENT2','EVENT3','EVENT4','EVENT5','EVENT6','EVENT7','EVENT8','EVENT9','EVENT10','EVENT11','EVENT12','EVENT13']\n",
    "for _i,event in enumerate(list_events):\n",
    "    cut_source_region_from_gradient_gauss(event,iteration, 600)\n",
    "sum_gradient(list_events,iteration, smoothsrc='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"SYN2_summed_gradient.dat\"\n",
    "#!cp OUTPUT_FILES/rho_alpha_beta_kernel.dat OUTPUT_FILES_bk/rho_alpha_beta_kernel_cc.dat \n",
    "plot_kernels(filename,show=True,EVENT='summed2_nosmooth',total_max=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration='SYN2'\n",
    "smooth_kernel(iteration+'_summed_gradient.dat',iteration,5000,5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"SYN2_summed_gradient_smooth.dat\"\n",
    "plot_kernels(filename,show=True,EVENT='summed2_nosmooth',total_max=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"SYN_summed_gradient_smooth.dat\"\n",
    "plot_kernels(filename,show=True,EVENT='summed2_nosmooth',total_max=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_1dmodel='profile_syn_numpy.xyz'\n",
    "finalmodel='profile_obs_numpy.xyz'\n",
    "filename='MODELS/profile_syn_iteration2_noheader_relative.xyz'\n",
    "plot_relative(initial_1dmodel,finalmodel,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###smooth kernel \n",
    "\n",
    "iteration='SYN'\n",
    "smooth_kernel(iteration+'_summed_gradient.dat',iteration,10000,10000)\n",
    "plot_kernels(iteration+'_summed_gradient_smooth.dat',show=True,EVENT='summed2_nosmooth',total_max=2e-9)\n",
    "\n",
    "iteration='SYN2'\n",
    "smooth_kernel(iteration+'_summed_gradient.dat',iteration,10000,10000)\n",
    "plot_kernels(iteration+'_summed_gradient_smooth.dat',show=True,EVENT='summed2_nosmooth',total_max=2e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_lbfgs(listofgradients,listofmodels):\n",
    "    import h5py\n",
    "    from lasif import function_store\n",
    "    from scipy.interpolate import griddata\n",
    "    x,z,rho,alpha,belta=read_gradient(listofgradients[0])\n",
    "    x_new,z_new,vp,vs,rho=read_modelfile(listofmodels[0])\n",
    "    fields=['vp','vs','rho']\n",
    "    ###becareful the shape of model and gradient is differen\n",
    "    g=np.zeros((len(listofgradients),3,x_new.shape[0]))\n",
    "    m=np.zeros((len(listofgradients),3,x_new.shape[0]))\n",
    "    modelupdate=np.zeros((3,x_new.shape[0]))\n",
    "    for i in np.arange(len(listofgradients)):\n",
    "        gradient=interpolate_kernel_format(listofgradients[i],x_new,z_new)\n",
    "        x_new,z_new,vp,vs,rho =read_modelfile(listofmodels[i])\n",
    "        model=np.zeros((3,x_new.shape[0]))\n",
    "        model[0]=vp\n",
    "        model[1]=vs\n",
    "        model[2]=rho\n",
    "        g[i]=gradient\n",
    "        m[i]=model\n",
    "    g=np.swapaxes(g,0,1)\n",
    "    m=np.swapaxes(m,0,1)\n",
    "    \n",
    "    ##swap axes for loop the fields\n",
    "    for _i, field in enumerate(fields):\n",
    "        modelupdate[_i]=function_store.LBFGS_n_order(g[_i],m[_i])\n",
    "    return modelupdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listofgradients=['SYN_summed_gradient_smooth.dat', 'SYN2_summed_gradient_smooth.dat']\n",
    "listofmodels=['profile_syn_numpy.xyz','profile_syn_iteration2_noheader.xyz']\n",
    "percent_vp=1\n",
    "percent_vs=0.01\n",
    "percent_rho=0.01\n",
    "smoothfactor_x=10000\n",
    "smoothfactor_z=10000\n",
    "newfile='MODELS/profile_syn_iteration3.xyz'\n",
    "newfile_noheader='MODELS/profile_syn_iteration3_noheader.xyz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_update_lbfgs(listofgradients,listofmodels,percent_vp,percent_vs,percent_rho,smoothfactor_x,smoothfactor_z,newfile,newfile_noheader):\n",
    "    model_update=assemble_lbfgs(listofgradients,listofmodels)\n",
    "    print('finish lbfgs')\n",
    "    x_new,z_new,vp,vs,rho=read_modelfile(listofmodels[-1])\n",
    "    smooth_format=smooth_compat(x_new,z_new,model_update[0],model_update[1],model_update[2],smoothfactor_x,smoothfactor_z)\n",
    "    print('finish smoothing')\n",
    "    vp_new=vp-percent_vp*smooth_format[0]\n",
    "    vs_new=vs-percent_vs*smooth_format[1]\n",
    "    rho_new=rho-percent_rho*smooth_format[2]\n",
    "    print(np.max(np.abs(percent_vp*smooth_format[0])),np.max(percent_vs*smooth_format[1]),np.max(percent_rho*smooth_format[2]))\n",
    "    generate_newmodel(vp_new,vs_new,rho_new,x_new,z_new,newfile,newfile_noheader)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_models('MODELS/profile_syn_iteration2_noheader.xyz',show=True,EVENT='model',total_max=11000,total_min=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_update_lbfgs(listofgradients,listofmodels,percent_vp,percent_vs,percent_rho,smoothfactor_x,smoothfactor_z,newfile,newfile_noheader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfile=PROJECT+'MODELS/'+'profile_syn_iteration3.xyz'\n",
    "list_events=['EVENT1','EVENT2','EVENT3','EVENT4','EVENT5','EVENT6','EVENT7','EVENT8','EVENT9','EVENT10','EVENT11','EVENT12','EVENT13']\n",
    "for _i, event in enumerate(list_events):\n",
    "    launch_forward(PROJECT,event, 'forward' ,modelfile,'REF_SEM_SYN3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_events=['EVENT1','EVENT2','EVENT3','EVENT4','EVENT5','EVENT6','EVENT7','EVENT8','EVENT9','EVENT10','EVENT11','EVENT12','EVENT13']\n",
    "for _i, event in enumerate(list_events):\n",
    "    adj_calculate(PROJECT,event,'REF_SEM/','REF_SEM_SYN3/','ccc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_events=['EVENT1','EVENT2','EVENT3','EVENT4','EVENT5','EVENT6','EVENT7','EVENT8','EVENT9','EVENT10','EVENT11','EVENT12','EVENT13']\n",
    "retrieve_misfit(list_events,'SYN3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_events=['EVENT1','EVENT2','EVENT3','EVENT4','EVENT5','EVENT6','EVENT7','EVENT8','EVENT9','EVENT10','EVENT11','EVENT12','EVENT13']\n",
    "retrieve_misfit(list_events,'SYN2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_events=['EVENT1','EVENT2','EVENT3','EVENT4','EVENT5','EVENT6','EVENT7','EVENT8','EVENT9','EVENT10','EVENT11','EVENT12','EVENT13']\n",
    "misfit1=retrieve_misfit(list_events,'SYN')\n",
    "misfit2=retrieve_misfit(list_events,'SYN2')\n",
    "misfit3=retrieve_misfit(list_events,'SYN3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.array([misfit1,misfit2,misfit3])/misfit1,'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_events=['EVENT1','EVENT2','EVENT3','EVENT4','EVENT5','EVENT6','EVENT7','EVENT8','EVENT9','EVENT10','EVENT11','EVENT12','EVENT13']\n",
    "for _i, event in enumerate(list_events):+\n",
    "    launch_adjoint(PROJECT,event,'REF_SEM_SYN3/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_models('MODELS/profile_obs_numpy.xyz',show=True,EVENT='model',total_max=11000,total_min=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_models('MODELS/profile_syn_iteration4_noheader.xyz',show=True,EVENT='model',total_max=11000,total_min=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_models('MODELS/profile_syn_iteration5_noheader.xyz',show=True,EVENT='model',total_max=11000,total_min=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration='SYN3'\n",
    "list_events=['EVENT1','EVENT2','EVENT3','EVENT4','EVENT5','EVENT6','EVENT7','EVENT8','EVENT9','EVENT10','EVENT11','EVENT12','EVENT13']\n",
    "for _i,event in enumerate(list_events):\n",
    "    cut_source_region_from_gradient_gauss(event,iteration, 600)\n",
    "sum_gradient(list_events,iteration, smoothsrc='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"SYN3_summed_gradient.dat\"\n",
    "#!cp OUTPUT_FILES/rho_alpha_beta_kernel.dat OUTPUT_FILES_bk/rho_alpha_beta_kernel_cc.dat \n",
    "plot_kernels(filename,show=True,EVENT='summed2_nosmooth',total_max=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration='SYN3'\n",
    "smooth_kernel(iteration+'_summed_gradient.dat',iteration,10000,10000)\n",
    "plot_kernels(iteration+'_summed_gradient_smooth.dat',show=True,EVENT='summed3_nosmooth',total_max=2e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listofgradients=['SYN_summed_gradient_smooth.dat', 'SYN2_summed_gradient_smooth.dat','SYN3_summed_gradient_smooth.dat']\n",
    "listofmodels=['profile_syn_numpy.xyz','profile_syn_iteration2_noheader.xyz','profile_syn_iteration3_noheader.xyz']\n",
    "percent_vp=1\n",
    "percent_vs=0.01\n",
    "percent_rho=0.01\n",
    "smoothfactor_x=10000\n",
    "smoothfactor_z=10000\n",
    "newfile='MODELS/profile_syn_iteration4.xyz'\n",
    "newfile_noheader='MODELS/profile_syn_iteration4_noheader.xyz'\n",
    "model_update_lbfgs(listofgradients,listofmodels,percent_vp,percent_vs,percent_rho,smoothfactor_x,smoothfactor_z,newfile,newfile_noheader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listofmodels=['profile_syn_numpy.xyz','profile_syn_iteration2_noheader.xyz','profile_syn_iteration3_noheader.xyz']\n",
    "test=np.loadtxt('MODELS/'+listofmodels[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfile=PROJECT+'MODELS/'+'profile_syn_iteration4.xyz'\n",
    "list_events=['EVENT1','EVENT2','EVENT3','EVENT4','EVENT5','EVENT6','EVENT7','EVENT8','EVENT9','EVENT10','EVENT11','EVENT12','EVENT13']\n",
    "for _i, event in enumerate(list_events):\n",
    "    launch_forward(PROJECT,event, 'forward' ,modelfile,'REF_SEM_SYN4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_events=['EVENT1','EVENT2','EVENT3','EVENT4','EVENT5','EVENT6','EVENT7','EVENT8','EVENT9','EVENT10','EVENT11','EVENT12','EVENT13']\n",
    "for _i, event in enumerate(list_events):\n",
    "    adj_calculate(PROJECT,event,'REF_SEM/','REF_SEM_SYN4/','ccc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misfit4=retrieve_misfit(list_events,'SYN4')\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.array([misfit1,misfit2,misfit3,misfit4])/misfit1,'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misfit4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_events=['EVENT1','EVENT2','EVENT3','EVENT4','EVENT5','EVENT6','EVENT7','EVENT8','EVENT9','EVENT10','EVENT11','EVENT12','EVENT13']\n",
    "for _i, event in enumerate(list_events):\n",
    "    launch_adjoint(PROJECT,event,'REF_SEM_SYN4/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration='SYN4'\n",
    "list_events=['EVENT1','EVENT2','EVENT3','EVENT4','EVENT5','EVENT6','EVENT7','EVENT8','EVENT9','EVENT10','EVENT11','EVENT12','EVENT13']\n",
    "for _i,event in enumerate(list_events):\n",
    "    cut_source_region_from_gradient_gauss(event,iteration, 600)\n",
    "sum_gradient(list_events,iteration, smoothsrc='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"SYN4_summed_gradient.dat\"\n",
    "#!cp OUTPUT_FILES/rho_alpha_beta_kernel.dat OUTPUT_FILES_bk/rho_alpha_beta_kernel_cc.dat \n",
    "plot_kernels(filename,show=True,EVENT='summed2_nosmooth',total_max=5e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration='SYN4'\n",
    "smooth_kernel(iteration+'_summed_gradient.dat',iteration,8000,8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listofgradients=['SYN_summed_gradient_smooth.dat', 'SYN2_summed_gradient_smooth.dat','SYN3_summed_gradient_smooth.dat','SYN4_summed_gradient_smooth.dat']\n",
    "listofmodels=['profile_syn_numpy.xyz','profile_syn_iteration2_noheader.xyz','profile_syn_iteration3_noheader.xyz','profile_syn_iteration4_noheader.xyz']\n",
    "percent_vp=1\n",
    "percent_vs=0.01\n",
    "percent_rho=0.01\n",
    "smoothfactor_x=8000\n",
    "smoothfactor_z=8000\n",
    "newfile='MODELS/profile_syn_iteration5.xyz'\n",
    "newfile_noheader='MODELS/profile_syn_iteration5_noheader.xyz'\n",
    "model_update_lbfgs(listofgradients,listofmodels,percent_vp,percent_vs,percent_rho,smoothfactor_x,smoothfactor_z,newfile,newfile_noheader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfile=PROJECT+'MODELS/'+'profile_syn_iteration5.xyz'\n",
    "list_events=['EVENT1','EVENT2','EVENT3','EVENT4','EVENT5','EVENT6','EVENT7','EVENT8','EVENT9','EVENT10','EVENT11','EVENT12','EVENT13']\n",
    "for _i, event in enumerate(list_events):\n",
    "    launch_forward(PROJECT,event, 'forward' ,modelfile,'REF_SEM_SYN5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_events=['EVENT1','EVENT2','EVENT3','EVENT4','EVENT5','EVENT6','EVENT7','EVENT8','EVENT9','EVENT10','EVENT11','EVENT12','EVENT13']\n",
    "for _i, event in enumerate(list_events):\n",
    "    adj_calculate(PROJECT,event,'REF_SEM/','REF_SEM_SYN5/','ccc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misfit5=retrieve_misfit(list_events,'SYN5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misfit5=retrieve_misfit(list_events,'SYN5')\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.array([misfit1,misfit2,misfit3,misfit4,misfit5])/misfit1,'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misfit1/60/13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_events=['EVENT1','EVENT2','EVENT3','EVENT4','EVENT5','EVENT6','EVENT7','EVENT8','EVENT9','EVENT10','EVENT11','EVENT12','EVENT13']\n",
    "for _i, event in enumerate(list_events):\n",
    "    launch_adjoint(PROJECT,event,'REF_SEM_SYN5/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration='SYN5'\n",
    "for _i,event in enumerate(list_events):\n",
    "    cut_source_region_from_gradient_gauss(event,iteration, 600)\n",
    "sum_gradient(list_events,iteration, smoothsrc='True')\n",
    "filename=\"SYN5_summed_gradient.dat\"\n",
    "#!cp OUTPUT_FILES/rho_alpha_beta_kernel.dat OUTPUT_FILES_bk/rho_alpha_beta_kernel_cc.dat \n",
    "plot_kernels(filename,show=True,EVENT='summed5_nosmooth',total_max=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"SYN_summed_gradient.dat\"\n",
    "plot_kernels(filename,show=True,EVENT='summed5_nosmooth',total_max=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration='SYN5'\n",
    "smooth_kernel(iteration+'_summed_gradient.dat',iteration,8000,8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listofgradients=['SYN_summed_gradient_smooth.dat', 'SYN2_summed_gradient_smooth.dat','SYN3_summed_gradient_smooth.dat','SYN4_summed_gradient_smooth.dat','SYN5_summed_gradient_smooth.dat']\n",
    "listofmodels=['profile_syn_numpy.xyz','profile_syn_iteration2_noheader.xyz','profile_syn_iteration3_noheader.xyz','profile_syn_iteration4_noheader.xyz','profile_syn_iteration5_noheader.xyz']\n",
    "percent_vp=1\n",
    "percent_vs=0.01\n",
    "percent_rho=0.01\n",
    "smoothfactor_x=8000\n",
    "smoothfactor_z=8000\n",
    "newfile='MODELS/profile_syn_iteration6.xyz'\n",
    "newfile_noheader='MODELS/profile_syn_iteration6_noheader.xyz'\n",
    "model_update_lbfgs(listofgradients,listofmodels,percent_vp,percent_vs,percent_rho,smoothfactor_x,smoothfactor_z,newfile,newfile_noheader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfile=PROJECT+'MODELS/'+'profile_syn_iteration6.xyz'\n",
    "list_events=['EVENT1','EVENT2','EVENT3','EVENT4','EVENT5','EVENT6','EVENT7','EVENT8','EVENT9','EVENT10','EVENT11','EVENT12','EVENT13']\n",
    "for _i, event in enumerate(list_events):\n",
    "    launch_forward(PROJECT,event, 'forward' ,modelfile,'REF_SEM_SYN6')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_events=['EVENT1','EVENT2','EVENT3','EVENT4','EVENT5','EVENT6','EVENT7','EVENT8','EVENT9','EVENT10','EVENT11','EVENT12','EVENT13']\n",
    "for _i, event in enumerate(list_events):\n",
    "    adj_calculate(PROJECT,event,'REF_SEM/','REF_SEM_SYN6/','ccc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misfit5=retrieve_misfit(list_events,'SYN5')\n",
    "misfit6=retrieve_misfit(list_events,'SYN6')\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.array([misfit1,misfit2,misfit3,misfit4,misfit5,misfit6])/misfit1,'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_events=['EVENT1','EVENT2','EVENT3','EVENT4','EVENT5','EVENT6','EVENT7','EVENT8','EVENT9','EVENT10','EVENT11','EVENT12','EVENT13']\n",
    "for _i, event in enumerate(list_events):\n",
    "    launch_adjoint(PROJECT,event,'REF_SEM_SYN6/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration='SYN6'\n",
    "for _i,event in enumerate(list_events):\n",
    "    cut_source_region_from_gradient_gauss(event,iteration, 600)\n",
    "sum_gradient(list_events,iteration, smoothsrc='True')\n",
    "filename=\"SYN6_summed_gradient.dat\"\n",
    "#!cp OUTPUT_FILES/rho_alpha_beta_kernel.dat OUTPUT_FILES_bk/rho_alpha_beta_kernel_cc.dat \n",
    "plot_kernels(filename,show=True,EVENT='summed6_nosmooth',total_max=5e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_relative(initial_1dmodel,finalmodel,filename):\n",
    "    x,z, vp,vs,rho=read_modelfile(initial_1dmodel)\n",
    "    x_new,z_new, vp_new,vs_new,rho_new=read_modelfile(finalmodel)\n",
    "    relative_vp=(vp_new-vp)/vp*100\n",
    "    relative_vs=(vs_new-vs)/vs*100\n",
    "    relative_rho=(rho_new-rho)/rho*100\n",
    "    relative=np.zeros((5,x.shape[0]))\n",
    "    relative[0]=x\n",
    "    relative[1]=z\n",
    "    relative[2]=relative_vp\n",
    "    relative[3]=relative_vs\n",
    "    relative[4]=relative_rho\n",
    "    np.savetxt(filename,relative.T)\n",
    "    plot_models(filename,show=True,EVENT='reltivemodel',total_max=3,total_min=-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_1dmodel='profile_syn_numpy.xyz'\n",
    "finalmodel='profile_syn_iteration6.xyz'\n",
    "filename='MODELS/profile_syn_iteration6_noheader_relative.xyz'\n",
    "plot_relative(initial_1dmodel,finalmodel,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_1dmodel='profile_syn_numpy.xyz'\n",
    "finalmodel='profile_syn_iteration2_noheader.xyz'\n",
    "filename='MODELS/profile_syn_iteration6_noheader_relative.xyz'\n",
    "plot_relative(initial_1dmodel,finalmodel,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"SYN6_summed_gradient_smooth.dat\"\n",
    "plot_kernels(filename,show=True,EVENT='summed6_nosmooth',total_max=2e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration='SYN6'\n",
    "smooth_kernel(iteration+'_summed_gradient.dat',iteration,6000,6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listofgradients=['SYN_summed_gradient_smooth.dat', 'SYN2_summed_gradient_smooth.dat','SYN3_summed_gradient_smooth.dat','SYN4_summed_gradient_smooth.dat',\\\n",
    "                 'SYN5_summed_gradient_smooth.dat','SYN6_summed_gradient_smooth.dat']\n",
    "listofmodels=['profile_syn_numpy.xyz','profile_syn_iteration2_noheader.xyz','profile_syn_iteration3_noheader.xyz'\\\n",
    "              ,'profile_syn_iteration4_noheader.xyz','profile_syn_iteration5_noheader.xyz','profile_syn_iteration6_noheader.xyz']\n",
    "percent_vp=1\n",
    "percent_vs=0.01\n",
    "percent_rho=0.01\n",
    "smoothfactor_x=6000\n",
    "smoothfactor_z=6000\n",
    "newfile='MODELS/profile_syn_iteration7.xyz'\n",
    "newfile_noheader='MODELS/profile_syn_iteration7_noheader.xyz'\n",
    "model_update_lbfgs(listofgradients,listofmodels,percent_vp,percent_vs,percent_rho,smoothfactor_x,smoothfactor_z,newfile,newfile_noheader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfile=PROJECT+'MODELS/'+'profile_syn_iteration7.xyz'\n",
    "list_events=['EVENT1','EVENT2','EVENT3','EVENT4','EVENT5','EVENT6','EVENT7','EVENT8','EVENT9','EVENT10','EVENT11','EVENT12','EVENT13']\n",
    "for _i, event in enumerate(list_events):\n",
    "    launch_forward(PROJECT,event, 'forward' ,modelfile,'REF_SEM_SYN7')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_events=['EVENT1','EVENT2','EVENT3','EVENT4','EVENT5','EVENT6','EVENT7','EVENT8','EVENT9','EVENT10','EVENT11','EVENT12','EVENT13']\n",
    "for _i, event in enumerate(list_events):\n",
    "    adj_calculate(PROJECT,event,'REF_SEM/','REF_SEM_SYN7/','ccc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misfit7=retrieve_misfit(list_events,'SYN7')\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.array([misfit1,misfit2,misfit3,misfit4,misfit5,misfit6,misfit7])/misfit1,'o')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
